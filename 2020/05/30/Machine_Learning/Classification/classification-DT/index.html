<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Yilin">


    <meta name="subtitle" content="I love data science">


    <meta name="description" content="the blog used to share projects and focus on AI industry">


    <meta name="keywords" content="machine learning,deep learning,statistics,data science">


<title>Classification - Decision Trees | Yilin&#39;s data science</title>






    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    





<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Yilin&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Yilin&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/categories">Categories</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Classification - Decision Trees</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Yilin</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">May 30, 2020&nbsp;&nbsp;14:31:26</a>
                        </span> 
                        

                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Machine-Learning/">Machine Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Decision tree is an important classification model. Decision tree has two different types depend on the target data type is belonging to continuous variable or discrete variable. And for the target which is discrete variable has three different main algorithms to build trees or split trees.</p>
<p><img src="/images/classfication_3/1.png" alt=""></p>
<h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><p>•    Definition</p>
<p>•    Data types of inputs and outputs</p>
<p>•    Two main types of decision trees</p>
<ol>
<li><p>Decision Trees: ID3/C4.5/CART</p>
</li>
<li><p>Regression Trees</p>
</li>
</ol>
<p>•    How to optimize model</p>
<p>•    Summary</p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Decision tree learning is one of the predictive modelling approaches used in statistics, data mining and machine learning. It uses a decision tree (as a predictive model) to go from the observation about training dataset and figure out the rules to split trees by different methods, for example, ID3 by using information gain to split trees, C4.5 by using gain ratio and CART by using Gini impurity.</p>
<p>There are two main types of decision trees. Tree models for the target which is discrete variable is <strong>Decision Tree</strong> and tree models for the target which is continuous variable is <strong>Regression Tree</strong>.</p>
<h2 id="Data-types-of-inputs-and-outputs"><a href="#Data-types-of-inputs-and-outputs" class="headerlink" title="Data types of inputs and outputs"></a>Data types of inputs and outputs</h2><p><strong>Inputs</strong>: Decision tree could handle both numerical and categorical data for input variables. And you don’t have to transfer the multiple categorical data into dummy variables. For example, you have a categorical feature which is color and this feature has three values which are red (0), green (1) and blue (2). For logistic regression and other models which are not tree models, you need to transfer this variable into dummy variables or three more columns which are Red, Green and Blue. And for each column only has two values which are 0 and 1.</p>
<p><strong>Output</strong>: The target for decision tree could be numerical and categorical as well. But for different data type for the output, it will generate different tree models. Numerical target is regression trees and categorical target is decision trees.</p>
<h2 id="Two-main-types-of-decision-trees-classification-trees-and-regression-trees"><a href="#Two-main-types-of-decision-trees-classification-trees-and-regression-trees" class="headerlink" title="Two main types of decision trees (classification trees and regression trees)"></a>Two main types of decision trees (classification trees and regression trees)</h2><h3 id="Decision-trees-ID3-C4-5-CART"><a href="#Decision-trees-ID3-C4-5-CART" class="headerlink" title="Decision trees: ID3/C4.5/CART"></a>Decision trees: ID3/C4.5/CART</h3><h4 id="ID3"><a href="#ID3" class="headerlink" title="ID3"></a>ID3</h4><h5 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h5><p>In decision tree learning, ID3(Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and it is typically used in the machine learning and natural language processing domains.</p>
<h5 id="Inputs-and-outputs"><a href="#Inputs-and-outputs" class="headerlink" title="Inputs and outputs"></a>Inputs and outputs</h5><p><strong>Input</strong>: The input for ID3 could be numerical and categorical data.</p>
<p><strong>Output</strong>: The output for ID3 could be numerical and categorical data. But if the output for the ID3 is numerical data which means the tree model is regression tree, it will consume large computation power. For that reason, ID3 is usually used for classification trees.</p>
<h5 id="Formula"><a href="#Formula" class="headerlink" title="Formula"></a>Formula</h5><p>There are two concepts you need to know for ID3 which are <strong>Entropy and Information Gain</strong>. </p>
<p>And there are four steps to build an ID3 tree by calculating Entropy and information gain:</p>
<ol>
<li>calculate <strong>Entropy</strong> for target and each feature.</li>
<li>get <strong>Information Gain</strong> between the entropy for target and entropy for each feature.</li>
<li>select the feature with largest information gain as the first node for ID3 tree.</li>
<li>repeat the process above for each node to form a tree.</li>
</ol>
<p><strong>1. Entropy</strong></p>
<p>Definition: </p>
<p>Entropy is nothing but the measure of disorder.</p>
<p><img src="/images/classfication_3/2.png" alt=""></p>
<p><strong>2. Information Gain</strong></p>
<p>Definition:</p>
<p><img src="/images/classfication_3/3.png" alt=""> </p>
<h5 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h5><p>I got the example for Entropy below from <a href="https://www.saedsayad.com/decision_tree.htm" target="_blank" rel="noopener">Decision Tree – Classification</a> </p>
<p>We have the dataset with features Outlook, Temp, Humidity and Windy. And we want to build an ID3 tree to make a prediction that whether we could go out to play golf or not.</p>
<p><img src="/images/classfication_3/4.png" alt=""> </p>
<p><strong>Step1</strong>: calculate <strong>Entropy</strong> for target and each feature</p>
<p>a) Entropy for <strong>target</strong>.</p>
<p><img src="/images/classfication_3/5.png" alt=""> </p>
<p>b) Entropy for <strong>each feature</strong> (In this part, I only put the Outlook this feature for example to show the process about how to calculate Entropy for features.).</p>
<p><img src="/images/classfication_3/6.png" alt="">  </p>
<p><strong>Step2</strong>: get <strong>Information Gain</strong> between the entropy for target and entropy for each feature.</p>
<p><img src="/images/classfication_3/7.png" alt="">  </p>
<p><strong>Step3</strong>: select the feature with largest information gain as the first node for ID3 tree.</p>
<p>From step2, we could observe that Outlook is the feature with the largest information gain and we put Outlook as the first node for the ID3 tree.</p>
<p><img src="/images/classfication_3/8.png" alt="">   </p>
<p><strong>Step4</strong>: repeat the process above for each node respectively to form a tree.</p>
<p><img src="/images/classfication_3/9.png" alt="">   </p>
<p><strong>Final ID3 tree for this example</strong>.</p>
<p><img src="/images/classfication_3/10.png" alt="">   </p>
<h4 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h4><h5 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h5><p>C4.5 is an algorithm used to generate decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan’s earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification. Actually C4.5 and ID3 is quite similar. For C4.5, it also needs to calculate Entropy and Information Gain for each feature. But after you got Entropy and Information gain, you need to calculate information gain ratio for each feature to reduce a bias towards multi-valued attributes by taking the number and size of branches into account when choosing an attribute. You may be confused about why we need to take the number and size of branches into account. No worries, I will explain more details in the example.</p>
<h5 id="Inputs-and-outputs-1"><a href="#Inputs-and-outputs-1" class="headerlink" title="Inputs and outputs"></a>Inputs and outputs</h5><p><strong>Input</strong>: The input for ID3 could be numerical and categorical data.</p>
<p><strong>Output</strong>: The output for C4.5 could be numerical and categorical data. But if the output for the C4.5 is numerical data which means the tree model is regression tree, it will consume large computation power. For that reason, C4.5 is usually used for classification trees just like ID3.</p>
<h5 id="Formula-1"><a href="#Formula-1" class="headerlink" title="Formula"></a>Formula</h5><p>There are three concepts you need to know for C4.5 which are <strong>Entropy, Information Gain and Information Gain Ratio</strong>. </p>
<p>And there are five steps to build an C4.5 tree by calculating Entropy and information gain:</p>
<ol>
<li>calculate <strong>Entropy</strong> for target and each feature.</li>
<li>get <strong>Information Gain</strong> between the entropy for target and entropy for each feature.</li>
<li>get <strong>Information Gain Ratio</strong> from Information gain divide by Intrinsic value for each feature.</li>
<li>select the feature with largest information gain ratio as the first node for C4.5 tree.</li>
<li>repeat the process above for each node to form a tree.</li>
</ol>
<p>You have already known what are entropy and information gain or splitinfo. What you need to know for C4.5 is intrinsic value for information gain ratio.</p>
<p><strong>Information Gain Ratio</strong></p>
<p><img src="/images/classfication_3/11.png" alt="">   </p>
<p><strong>Example/ Why we need gain ratio for C4.5</strong></p>
<p>C4.5 is very similar to the ID3. In this part, I will introduce an example and why we need gain ratio for C4.5.</p>
<p>We have the dataset with features PlayTime and Humidity. And we want to build an C4.5 tree to make a prediction that whether we could go out to play golf or not.</p>
<p><img src="/images/classfication_3/12.png" alt="">   </p>
<p><strong>Step1</strong>: calculate <strong>Entropy</strong> for target and each feature.</p>
<p>a) Entropy for <strong>target</strong>.</p>
<p><img src="/images/classfication_3/13.png" alt="">   </p>
<p>b) Entropy for <strong>each feature</strong> .<br>You could see the Entropy for each feature in the step2.</p>
<p><strong>Step2</strong>: get <strong>Information Gain</strong> between the entropy for target and entropy for each feature.</p>
<p><img src="/images/classfication_3/14.png" alt="">   </p>
<p><strong>Step3</strong>: get <strong>Information Gain Ratio</strong> from Information gain divide by Intrinsic value for each feature.</p>
<p>We could observe from the table above that the PlayTime is continuous variable and Humidity is discrete variable. The PlayTime has highest information gain and Humidity has lower information gain.</p>
<p>Here comes the problem. The reason why we need information gain is to find the best feature for each node to split the tree. But in this case, PlayTime will be the first node. That is a bad feature especially to be as the first node for the tree.  Because if the PlayTime as the first tree will make the tree very complex.</p>
<p>C4.5 introduce the gain ratio to solve the problem. You have already entropy for target and each feature and information gain for each feature. You still need to calculate split info which also called intrinsic value for each feature.</p>
<p><img src="/images/classfication_3/15.png" alt="">   </p>
<p>After calculating the information gain ratio, we could observe that the information gain ratio for the PlayTime is only 0.25 which is lower than the information gain ration for Humidity which is 0.71.</p>
<p><strong>Step4</strong>: select the feature with largest information gain ratio as the first node for C4.5 tree.</p>
<p>In this case, we select Humidity as the first.</p>
<p><strong>Step5</strong>: repeat the process above for each node to form a tree.</p>
<h4 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h4><p><strong>Definition</strong></p>
<p>CART (classification and Regression Trees) is introduced by Leo Breiman to refer to decision tree algorithms that can be used for classification or regression predictive modeling problems. Used by CART algorithm for classification trees, <strong>Gini impurity</strong> is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. </p>
<p><strong>Note</strong>: Unlike ID3 and C4.5, CART is a binary classification tree. (each feature with value YES/NO).</p>
<p><strong>Inputs and outputs</strong></p>
<p><strong>Input</strong>: The input for CART could be numerical and categorical data.</p>
<p><strong>Output</strong>: The output for CART could be numerical and categorical data. Unlike ID3 and C4.5 need to consume large computational power, CART could deal with continuous output very well. CART could be used to build classification tree and also regression tree based on the different types of outputs. I will introduce about CART regression tree more detail in the second part which is Regression Trees.</p>
<p><strong>Formula</strong></p>
<p><img src="/images/classfication_3/16.png" alt="">  </p>
<p>There is one concept you need to know for CART which is <strong>Gini Impurity</strong>. </p>
<p>And there are three steps to build a CART classification tree by calculating Gini impurity:</p>
<ol>
<li>calculate <strong>Gini Impurity</strong> for each feature.</li>
<li>compare Gini impurity for each feature and select the feature with minimum value as the first node for the CART tree.</li>
<li>repeat the process above for each node to form a CART tree until there is no point in separating the features anymore and it becomes a leaf node.</li>
</ol>
<p>I found the blog <a href="https://victorzhou.com/blog/gini-impurity/" target="_blank" rel="noopener">A Simple Explanation of Gini Impurity</a> by Victor Zhou explain the Gini impurity very well if you want to know more details about Gini Impurity.</p>
<p><strong>Examples/ How to calculate Gini impurity for different inputs</strong></p>
<p>There are four main different data type for input value which are binary categorical variable, multiple categorical variable, ranked variable and continuous variable. For different data type, there are different ways to calculate Gini impurity.</p>
<p>I got screenshots below in this part from <a href="https://www.youtube.com/watch?v=7VeUPuFGJHk" target="_blank" rel="noopener">StatQuest: Decision Trees</a>.</p>
<p> <em>Gini impurity for binary categorical variable</em></p>
<p>The sample data for this example has the features which are Chest Pain, Good Blood Circulation and Blocked Arteries. The target is Heart Disease.</p>
<p><img src="/images/classfication_3/17.png" alt="">  </p>
<p><strong>Step1</strong>: calculate <strong>Gini Impurity</strong> for each feature.</p>
<p>a) Example - Gini impurity for Chest Pain (Yes/No)</p>
<p>*<em>Gini impurity for “Yes” *</em></p>
<p><img src="/images/classfication_3/18.png" alt="">  </p>
<p>*<em>Gini impurity for “No” *</em></p>
<p><img src="/images/classfication_3/19.png" alt="">  </p>
<p><strong>Gini impurity for Chest Pain</strong></p>
<p><img src="/images/classfication_3/20.png" alt="">  </p>
<p>b) Gini impurity for each feature</p>
<p><img src="/images/classfication_3/21.png" alt="">  </p>
<p><strong>Step2</strong>: compare Gini impurity for each feature and select the feature with minimum value as the first node for the CART tree.</p>
<p>For the screenshot above, we could observe that Gini impurity for Good Blood Circulation has the lowest Gini impurity and we could put Good Blood Circulation as the first node for CART tree.</p>
<p><strong>Step3</strong>: repeat the process above for each node to form a CART tree.</p>
<p>After you split the tree by the first node which is Good Blood Circulation, we need to repeat the step1 for each split. From the screenshot below, we could observe that the Blocked Arteries will be the next node for the left split of Good Blood Circulation. For the reason that Blocked Arteries has the lowest Gini impurity compares with Chest Pain.</p>
<p><img src="/images/classfication_3/22.png" alt="">  </p>
<p><em>Gini impurity for multiple categorical variable</em></p>
<p>Assume we have a multiple variable with values like Green, Blue, Red and Green. We need to split the value into multiple group. </p>
<ol>
<li>Color Choice: Blue (is Blue or not)</li>
<li>Color Choice: Green (is Green or not)</li>
<li>Color Choice: Red (is Red or not)</li>
<li>Color Choice: Blue or Green (is (Blue or Green) or not)</li>
<li>Color Choice: Blue or Red (is (Blue or Red) or not)</li>
<li>Color Choice: Green or Red (is (Green or Red) or not)</li>
</ol>
<p><strong>Note</strong>: we don’t need to calculate an impurity score for “Color Choice: Blue or Green or Red” since that includes everyone.</p>
<p><img src="/images/classfication_3/23.png" alt="">  </p>
<p>*Gini impurity for continuous variable *</p>
<p>There have three steps to calculate the Gini impurity for continuous variable.</p>
<p>a) sort the patients by the feature which is weight in this example from lowest to highest.</p>
<p><img src="/images/classfication_3/24.png" alt="">  </p>
<p>b) calculate the average weight for all adjacent patient.</p>
<p><img src="/images/classfication_3/25.png" alt="">  </p>
<p>c) calculate the impurity values for each average weight.</p>
<p><img src="/images/classfication_3/26.png" alt="">  </p>
<p>*Gini impurity for ranked variable *</p>
<p>The way to calculate ranked variable is basically same to the continuous variable. Assume that we have a ranked variable with four values which are 1,2,3,4. </p>
<p><img src="/images/classfication_3/27.png" alt="">  </p>
<h4 id="Differences-among-ID3-C4-5-and-CART"><a href="#Differences-among-ID3-C4-5-and-CART" class="headerlink" title="Differences among ID3, C4.5 and CART"></a>Differences among ID3, C4.5 and CART</h4><p><strong>Algorithms to split the tree</strong>:</p>
<p>ID3: Information Gain<br>C4.5: Gain Ratio<br>CART: Gini Impurity</p>
<p><strong>Inputs and outputs</strong>:</p>
<p>a) Input:<br>ID3: suggested to be categorical variable.<br>C4.5: categorical variable and continuous variable (because C4.5 solve the problem of ID3 that ID3 has the bias to the continuous variable).<br>CART: categorical variable and continuous variable. (there has different way to calculate different input data type: binary categorical variable, multiple categorical variable, ranked variable and continuous variables).</p>
<p>b) Output:<br>ID3: suggested to be categorical variable (computationally expensive to calculate continuous variable).<br>C4.5: suggested to be categorical variable (computationally expensive to calculate continuous variable).<br>CART: categorical variable (classification tree) and continuous variable (regression tree).</p>
<h3 id="Regression-Trees"><a href="#Regression-Trees" class="headerlink" title="Regression Trees"></a>Regression Trees</h3><p><strong>Definition</strong></p>
<p>CART could not only apply to build a classification tree, but also to build a regression tree which with the target is continuous variable. Instead calculate Gini Impurity, CART calculates the variance reduction for each feature to decide which node is chosen to split the tree.</p>
<p><strong>Inputs and outputs</strong></p>
<p><strong>Input</strong>: The input for CART could be numerical and categorical data.</p>
<p><strong>Output</strong>: The output for CART regression is continuous variable.</p>
<p><strong>Formula</strong></p>
<p><img src="/images/classfication_3/28.png" alt="">  </p>
<p>There is one concept you need to know for CART regression tree which is <strong>variance reduction</strong> instead of Gini impurity for CART classification tree.</p>
<p>And there are three steps to build a CART regression tree by variance reduction:</p>
<ol>
<li>calculate <strong>variance reduction</strong> for each feature.</li>
<li>compare variance reduction for each feature and select the feature with minimum value as the first node for the CART regression tree.</li>
<li>repeat the process above for each node to form a CART regression tree until there is no point in separating the features anymore and it becomes a leaf node.</li>
</ol>
<p><strong>Examples/ How to calculate variance reduction for different inputs</strong></p>
<p>I got screenshots below in this part from <a href="https://www.youtube.com/watch?v=g9c66TUylZ4" target="_blank" rel="noopener">Regression Trees, Clearly Explained!!!</a>. </p>
<p><strong>Step1</strong>: calculate <strong>variance reduction</strong> for each feature.</p>
<p>The sample data for this example has the features which are Dosage, Age and Sex. The target is continuous variable which is Drug Effect.</p>
<p><img src="/images/classfication_3/29.png" alt=""> </p>
<p>You could learn more detail about how to calculate the variance (SSR: Sum of Squared Residuals) for each feature from <a href="https://www.youtube.com/watch?v=g9c66TUylZ4" target="_blank" rel="noopener">Regression Trees, Clearly Explained!!!</a>. </p>
<p><img src="/images/classfication_3/30.png" alt=""> </p>
<p><strong>Step2</strong>: compare variance reduction for each feature and select the feature with maximum value as the first node for the CART regression tree.</p>
<p>Instead of calculating maximum of variance reduction, we could just calculate SSR for each feature and select the feature with the minimum SSR as the first node.</p>
<p>In this case, we could observe from the screenshot above that Age has the lowest SSR and we could select Age as the first node for the CART regression tree.</p>
<p><strong>Step3</strong>: repeat the process above for each node to form a CART regression tree until there is no point in separating the features anymore and it becomes a leaf node.</p>
<p>The screenshot is the final tree in this case.</p>
<p><img src="/images/classfication_3/31.png" alt=""> </p>
<p><a href="https://github.com/scikit-learn/scikit-learn/blob/master/doc/modules/tree.rst" target="_blank" rel="noopener">https://github.com/scikit-learn/scikit-learn/blob/master/doc/modules/tree.rst</a> </p>
<h2 id="Decision-tree-pruning"><a href="#Decision-tree-pruning" class="headerlink" title="Decision tree pruning"></a>Decision tree pruning</h2><p><a href="https://benalexkeen.com/wp-content/uploads/2017/05/tree.png" target="_blank" rel="noopener">https://benalexkeen.com/wp-content/uploads/2017/05/tree.png</a> </p>
<p>If we have multiple of features and we don’t set limitation to the decision tree, the decision tree will be too complex like the image below and will cause <strong>overfitting problem</strong>.</p>
<p><img src="/images/classfication_3/32.png" alt=""> </p>
<p><strong>Definition</strong></p>
<p>Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improve accuracy by reduction of overfitting. </p>
<p>And pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set.</p>
<p><strong>Techniques</strong></p>
<p>Pruning can occur in a top down or bottom up fashion. A top down pruning (pre-pruning) will traverse nodes and trim subtrees starting at the root, while bottom up pruning (post pruning) will start at the leaf nodes.</p>
<ol>
<li>pre-pruning</li>
</ol>
<p>1.1 Set threshold for decision tree</p>
<p>We could set threshold for tree height/ information gain or gain ratio (for ID3 and C4.5) / object of node.</p>
<ol start="2">
<li>post pruning</li>
</ol>
<p>2.1 Reduced error pruning</p>
<p>•    Consider each node for pruning<br>•    Pruning = removing the subtree at that node, make it a leaf and assign the most common class at that node<br>•    A node is removed if the resulting tree performs no worse then the original on the validation set - removes coincidences and errors<br>•    Nodes are removed iteratively choosing the node whose removal most increases the decision tree accuracy on the graph<br>•    Pruning continues until further pruning is harmful<br>•    Uses training, validation &amp; test sets - effective approach if a large amount of data is available</p>
<p>2.2 Cost-complexity pruning</p>
<p>You could take a look at <a href="http://mlwiki.org/index.php/Cost-Complexity_Pruning" target="_blank" rel="noopener">Cost-Complexity Pruning</a> and <a href="https://www.youtube.com/watch?v=D0efHEJsfHo" target="_blank" rel="noopener">How to Prune Regression Trees, Clearly Explained!!!</a> for more detail about cost complexity pruning.</p>
<p>Algorithm:</p>
<p><img src="/images/classfication_3/33.png" alt=""> </p>
<p>Basically, cost-complexity pruning prunes the tree from bottom to up one node by one node and add penalty which is learning rate*the complexity of tree to evaluate from which node to prune will have lower cost.</p>
<h2 id="Advantages-of-decision-tree"><a href="#Advantages-of-decision-tree" class="headerlink" title="Advantages of decision tree"></a>Advantages of decision tree</h2><p>•    Simple to understand and interpret.<br>•    Require little data preparation. For the reason that decision tree able to handle both numerical and categorical data without to transfer them into dummy variables.</p>
<h2 id="Disadvantages-of-decision-tree"><a href="#Disadvantages-of-decision-tree" class="headerlink" title="Disadvantages of decision tree"></a>Disadvantages of decision tree</h2><p>•    Trees can be very non-robust. A small change in the training data can result in a large change in the tree and consequently the final predictions.<br>•    Easily to cause over-fitting problem.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>I hope you could have a basic understanding about decision tree from this post. Basically, I introduced two main type of decision trees which are classification trees and regression trees. For classification tree, there are three main models which are ID3, C4.5 and CART. Decision tree has a very serious problem which is very easily to be overfitting. That’s is reason why we need to prune the tree to avoid tree to be complex. </p>
<h5 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h5><p><a href="https://www.thelearningmachine.ai/tree-id3" target="_blank" rel="noopener">https://www.thelearningmachine.ai/tree-id3</a><br><a href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/ID3_algorithm</a><br><a href="https://www.saedsayad.com/decision_tree.htm" target="_blank" rel="noopener">https://www.saedsayad.com/decision_tree.htm</a><br><a href="https://en.wikipedia.org/wiki/Information_gain_ratio" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Information_gain_ratio</a><br><a href="https://www.youtube.com/watch?v=7VeUPuFGJHk" target="_blank" rel="noopener">https://www.youtube.com/watch?v=7VeUPuFGJHk</a><br><a href="https://blog.csdn.net/weixin_36586536/article/details/80468426" target="_blank" rel="noopener">https://blog.csdn.net/weixin_36586536/article/details/80468426</a><br><a href="https://en.wikipedia.org/wiki/Decision_tree_pruning" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Decision_tree_pruning</a><br><a href="https://pdfs.semanticscholar.org/025b/8c109c38dc115024e97eb0ede5ea873fffdb.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/025b/8c109c38dc115024e97eb0ede5ea873fffdb.pdf</a><br><a href="https://www.cs.auckland.ac.nz/~pat/706_98/ln/node90.html" target="_blank" rel="noopener">https://www.cs.auckland.ac.nz/~pat/706_98/ln/node90.html</a><br><a href="http://mlwiki.org/index.php/Cost-Complexity_Pruning" target="_blank" rel="noopener">http://mlwiki.org/index.php/Cost-Complexity_Pruning</a> </p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Yilin</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://yoursite.com/2020/05/30/Machine_Learning/Classification/classification-DT/">http://yoursite.com/2020/05/30/Machine_Learning/Classification/classification-DT/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>I love <strong>Data Science</strong></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Machine-Learning/"># Machine Learning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2020/05/23/Machine_Learning/Classification/classification-Introduction/">Classification Introduction</a>
            

        </section>
         <p>
         <p>
         <p>

       <section class="post-copyright">
         <p>
         <p>
         <p>
         <div id="disqus_thread"></div>
       </section>



    </article>


  
</div>




<section id="comments">

<script>
  var disqus_shortname = 'http-www-yilinhuang-cool';
  
  var disqus_url = 'http://yoursite.com/2020/05/30/Machine_Learning/Classification/classification-DT/';
  

(function() {
var d = document, s = d.createElement('script');
s.src = 'https://http-www-yilinhuang-cool.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
</section>

















        </div>
        
<footer id="footer" class="footer">


    <div class="copyright">
        <span>© Yilin | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
  
    </div>

</footer>











    </div>
</body>
</html>
